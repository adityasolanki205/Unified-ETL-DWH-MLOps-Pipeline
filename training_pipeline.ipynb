{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0b0894-1e05-4590-80fd-4c4862b42ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    component,\n",
    "    Metrics,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Artifact,\n",
    "    OutputPath,\n",
    "    Output,\n",
    ")\n",
    "from kfp import compiler\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb01a698-9650-4553-bf7a-1f733e78b787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "def data_ingestion(input_data_path: str,\n",
    "                   project_id: str,\n",
    "                   region: str, \n",
    "                   input_data: Output[Dataset],):\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    client = bigquery.Client(project=project_id, location=region)\n",
    "    sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `{}.GermanCredit.GermanCreditTable`\n",
    "    \"\"\".format(project_id)\n",
    "    # sql = f\"\"\"\n",
    "    # SELECT *\n",
    "    # FROM `{project_id}.GermanCredit.GermanCreditTable`\n",
    "    # \"\"\"\n",
    "    df = client.query_and_wait(sql).to_dataframe()\n",
    "    df.to_csv(input_data.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c9eabe5-804e-40c9-a0a3-cdf2f3447eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    "# )\n",
    "# def data_ingestion(input_data_path: str, input_data: Output[Dataset],):\n",
    "#     import pandas as pd\n",
    "#     from datetime import datetime, timedelta\n",
    "#     from google.cloud import bigquery\n",
    "#     import logging\n",
    "#     df = pd.read_csv(input_data_path)\n",
    "#     df.to_csv(input_data.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd12324b-1b30-4e3d-be27-699c34fd971b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "\n",
    "def preprocessing(train_df: Input[Dataset], input_data_preprocessed: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import warnings\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    def encode_columns(df, columns):\n",
    "        encoders = {}\n",
    "        for column in columns:\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "            encoders[column] = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "        return df\n",
    "    def preprocess(df):\n",
    "        numeric_columns = df.describe().columns\n",
    "        df_log_transformed = df.copy()\n",
    "        df_log_transformed[numeric_columns] = df[numeric_columns].apply(lambda x: np.log(x + 1))\n",
    "        scaler = MinMaxScaler()\n",
    "        df_scaled_log_transformed = df_log_transformed.copy()\n",
    "        df_scaled_log_transformed[numeric_columns] = scaler.fit_transform(df_scaled_log_transformed[numeric_columns])\n",
    "        categorical_columns = [\n",
    "        'Existing_account', 'Credit_history', 'Purpose', 'Saving',\n",
    "        'Employment_duration', 'Personal_status', 'Debtors', 'Property',\n",
    "        'Installment_plans', 'Housing', 'Job', 'Telephone', 'Foreign_worker'\n",
    "        ]\n",
    "        df_scaled_log_transformed = encode_columns(df_scaled_log_transformed, categorical_columns)\n",
    "        return df_scaled_log_transformed\n",
    "\n",
    "    df = pd.read_csv(train_df.path)\n",
    "    df = preprocess(df)\n",
    "    df.to_csv(input_data_preprocessed.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "349b21ec-383d-4929-a583-ec80411e7aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "def train_test_data_split(\n",
    "    dataset_in: Input[Dataset],\n",
    "    target_column: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "    test_size: float = 0.2,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    from sklearn.utils import shuffle\n",
    "    def get_train_test_splits(df, target_column, test_size_sample ):\n",
    "        df = shuffle(df)\n",
    "        x = df.drop(target_column, axis=1)\n",
    "        y = df[target_column]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = test_size_sample)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        y_test = pd.DataFrame(y_test)\n",
    "        X_train.reset_index(drop=True, inplace=True)\n",
    "        y_train.reset_index(drop=True, inplace=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        X_train = pd.concat([X_train, y_train], axis=1)\n",
    "        X_test = pd.concat([X_test, y_test], axis=1)\n",
    "        X_train.columns = x.columns.to_list() + [target_column]\n",
    "        X_test.columns = x.columns.to_list() + [target_column]\n",
    "        return X_train, X_test\n",
    "    data = pd.read_csv(dataset_in.path)\n",
    "    X_train, X_test = get_train_test_splits(\n",
    "        data, target_column, test_size\n",
    "    )\n",
    "    X_train.to_csv(dataset_train.path, index=False)\n",
    "    X_test.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d8ad7f4-cde9-4902-8465-8a29dd9b337f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "     base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "def hyperparameters_training(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    target: str,\n",
    "    max_evals: int,\n",
    "    metrics: Output[Metrics],\n",
    "    param_artifact: Output[Artifact],\n",
    "    ml_model: Output[Model],\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "    import joblib\n",
    "    import os\n",
    "    import json\n",
    "    import logging\n",
    "    \n",
    "\n",
    "    X_train = pd.read_csv(dataset_train.path)\n",
    "    X_test = pd.read_csv(dataset_test.path)\n",
    "\n",
    "    y_train = X_train[target]\n",
    "    y_test = X_test[target]\n",
    "    X_train = X_train.drop(target, axis=1)\n",
    "    X_test = X_test.drop(target, axis=1)\n",
    "    space = {\n",
    "        'C': hp.loguniform('C', -3, 3),  # log-uniform between ~0.05 to ~20\n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2']),  # safer to exclude 'elasticnet' unless solver == 'saga'\n",
    "        'solver': hp.choice('solver', ['liblinear', 'saga']),  # only solvers that support l1\n",
    "        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "        'max_iter': hp.choice('max_iter', [100, 1000,2500, 5000]),\n",
    "    }\n",
    "    def objective(params):\n",
    "        rf = LogisticRegression(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        metrics.log_metric(\"accuracy\", accuracy)\n",
    "        metrics.log_metric(\"precision\", precision)\n",
    "        metrics.log_metric(\"recall\", recall)\n",
    "        metrics.log_metric(\"f1\", f1)\n",
    "\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK, 'model': rf}\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "\n",
    "    best_params = trials.best_trial['result']['model'].get_params()\n",
    "    best_model = trials.best_trial['result']['model']\n",
    "\n",
    "    # Save the best model\n",
    "    os.makedirs(ml_model.path, exist_ok=True)\n",
    "    joblib.dump(best_model, os.path.join(ml_model.path, 'model.joblib'))\n",
    "\n",
    "    # Save the best hyperparameters\n",
    "    with open(param_artifact.path, \"w\") as f:\n",
    "        json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59f006fb-cebc-4671-8cbd-0e361e68c4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "def deploy_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    ml_model: Input[Model],\n",
    "    model_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    model_uri: Output[Artifact],\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    existing_models = aiplatform.Model.list(\n",
    "        filter=f\"display_name={model_name}\", project=project, location=region\n",
    "    )\n",
    "    if existing_models:\n",
    "        latest_model = existing_models[0]\n",
    "        logger.info(f\"Creating a new version for existing model: {latest_model.name}\")\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=model_name,\n",
    "            artifact_uri=ml_model.path,\n",
    "            location='asia-south1',\n",
    "            serving_container_image_uri=serving_container_image_uri,\n",
    "            parent_model=latest_model.resource_name,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"No existing model found. Creating a new model.\")\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=model_name,\n",
    "            artifact_uri=ml_model.path,\n",
    "            location='asia-south1',\n",
    "            serving_container_image_uri=serving_container_image_uri,\n",
    "        )\n",
    "    os.makedirs(model_uri.path, exist_ok=True)\n",
    "    with open(os.path.join(model_uri.path, \"model_uri.txt\"), \"w\") as f:\n",
    "        f.write(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6ccfa6b-ece1-4699-b315-76c738d793a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"asia-south1-docker.pkg.dev/solar-dialect-264808/kubeflow-pipelines/demo_model\"\n",
    ")\n",
    "def create_endpoint(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    model_uri: Input[Artifact],\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "    import os\n",
    "    with open(os.path.join(model_uri.path, \"model_uri.txt\"), \"r\") as f:\n",
    "        model_resource_name = f.read()\n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "    traffic_split = {\"0\": 100}\n",
    "    machine_type = \"n1-standard-4\"\n",
    "    min_replica_count = 1\n",
    "    max_replica_count = 1\n",
    "    \n",
    "    endpoint = model.deploy(\n",
    "            deployed_model_display_name=model_name,\n",
    "            machine_type=machine_type,\n",
    "            traffic_split = traffic_split,\n",
    "            min_replica_count=min_replica_count,\n",
    "            max_replica_count=max_replica_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77f98eda-e578-4bc6-9330-95c0bcc57f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"Training Pipeline\", pipeline_root=\"gs://demo_bucket_kfl/pipeline_root_demo\")\n",
    "def pipeline(\n",
    "    input_data_path: str = \"gs://demo_bucket_kfl/german_data.csv\",\n",
    "    project_id: str = \"solar-dialect-264808\",\n",
    "    region: str = \"asia-south1\",\n",
    "    model_name: str = \"demo_model\",\n",
    "    target: str = \"Classification\",\n",
    "    max_evals: int = 30,\n",
    "    use_hyperparameter_tuning: bool = True,\n",
    "    serving_container_image_uri: str = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest\"\n",
    "):\n",
    "    data_op = data_ingestion(\n",
    "        input_data_path=input_data_path,project_id=project_id, region=region)\n",
    "    data_op.set_caching_options(False)\n",
    "\n",
    "    data_preprocess_op = preprocessing(train_df=data_op.outputs[\"input_data\"])\n",
    "    data_preprocess_op.set_caching_options(False)\n",
    "    train_test_split_op = train_test_data_split(\n",
    "        dataset_in=data_preprocess_op.outputs[\"input_data_preprocessed\"],\n",
    "        target_column=\"Classification\",\n",
    "        test_size=0.2,\n",
    "    )\n",
    "    train_test_split_op.set_caching_options(False)\n",
    "    hyperparam_tuning_op = hyperparameters_training(\n",
    "        dataset_train=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        dataset_test=train_test_split_op.outputs[\"dataset_test\"],\n",
    "        target=target,\n",
    "        max_evals=max_evals\n",
    "    )\n",
    "    hyperparam_tuning_op.set_caching_options(False)\n",
    "    deploy_model_op = deploy_model(\n",
    "        project=project_id, region=region,\n",
    "        ml_model=hyperparam_tuning_op.outputs[\"ml_model\"],\n",
    "        model_name=model_name,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "    )\n",
    "    deploy_model_op.set_caching_options(False)\n",
    "    create_endpoint_op = create_endpoint(\n",
    "        project=project_id, region=region,\n",
    "        model_name=model_name,\n",
    "        model_uri = deploy_model_op.outputs[\"model_uri\"]\n",
    "    )\n",
    "    create_endpoint_op.set_caching_options(False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"training_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2dec9-1337-4e29-900f-b7deeb30dbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17df0f1-b026-44da-b360-4fb5b002d828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
